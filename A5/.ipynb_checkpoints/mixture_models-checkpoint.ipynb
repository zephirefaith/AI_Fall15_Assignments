{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 5: Gaussian mixture models\n",
    "\n",
    "In this assignment, you will learn to perform image segmentation through implementing Gaussian mixture models and iteratively improving their performance. You will be performing this segmentation on the \"Doge\" and the \"Party Spock\" images included with the assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "A Gaussian mixture model is a generative model for representing the underlying probability distribution of a complex collection of data, such as the collection of pixels in a grayscale photograph. \n",
    "\n",
    "In the context of this problem, a Gaussian mixture model defines the joint probability $f(x)$ as\n",
    "\n",
    "$$f(x) = \\sum_{i=1}^{k}m_{i}N_i(x |  \\mu_{i}, \\sigma_{i}^{2})$$\n",
    "\n",
    "where $x$ is a grayscale value [0,1], $f(x)$ is the joint probability of that gray scale value, $m_{i}$ is the mixing coefficient on component $i$, $N_i$ is the $i^{th}$ Gaussian distribution underlying the value $x$ with mean $\\mu_{i}$ and variance $\\sigma_{i}^{2}$. \n",
    "\n",
    "We will be using this model to segment photographs into different grayscale regions. The idea of segmentation is to assign a component $i$ to each pixel $x$ using the maximum posterior probability\n",
    "\n",
    "$$component_{x} = argmax_{i}(m_{i}N_i(x|\\mu_{i},\\sigma_{i}^{2})$$\n",
    "\n",
    "Then we will replace each pixel in the image with its corresponding $\\mu_{i}$ to produce a result as below (original above, segmented with three components below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Original image.](images/party_spock.png)\n",
    "![Post-segmentation.](images/party_spock3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import warnings\n",
    "warnings.simplefilter(action = \"ignore\", category = FutureWarning)\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from matplotlib import image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Helper image-processing code.\"\"\"\n",
    "def image_to_matrix(image_file, grays=False):\n",
    "    \"\"\"\n",
    "    Convert .png image to matrix\n",
    "    of values.\n",
    "    \n",
    "    params:\n",
    "    image_file = str\n",
    "    grays = Boolean\n",
    "    \n",
    "    returns:\n",
    "    img = (color) np.ndarray[np.ndarray[np.ndarray[float]]] \n",
    "    or (grayscale) np.ndarray[np.ndarray[float]]\n",
    "    \"\"\"\n",
    "    img = image.imread(image_file)\n",
    "    # in case of transparency values\n",
    "    if(len(img.shape) == 3 and img.shape[2] > 3):\n",
    "        height, width, depth = img.shape\n",
    "        new_img = np.zeros([height, width, 3])\n",
    "        for r in range(height):\n",
    "            for c in range(width):\n",
    "                new_img[r,c,:] = img[r,c,0:3]\n",
    "        img = np.copy(new_img)\n",
    "    if(grays and len(img.shape) == 3):\n",
    "        height, width = img.shape[0:2]\n",
    "        new_img = np.zeros([height, width])\n",
    "        for r in range(height):\n",
    "            for c in range(width):\n",
    "                new_img[r,c] = img[r,c,0]\n",
    "        img = new_img\n",
    "    # clean up zeros\n",
    "    if(len(img.shape) == 2):\n",
    "        zeros = np.where(img == 0)[0]\n",
    "        img[zeros] += 1e-7\n",
    "    return img\n",
    "\n",
    "def matrix_to_image(image_matrix, image_file):\n",
    "    \"\"\"\n",
    "    Convert matrix of color/grayscale \n",
    "    values  to .png image\n",
    "    and save to file.\n",
    "    \n",
    "    params:\n",
    "    image_matrix = (color) numpy.ndarray[numpy.ndarray[numpy.ndarray[float]]] or (grayscale) numpy.ndarray[numpy.ndarray[float]]\n",
    "    image_file = str\n",
    "    \"\"\"\n",
    "    # provide cmap to grayscale images\n",
    "    cMap = None\n",
    "    #print image_matrix.shape\n",
    "    len(image_matrix.shape)\n",
    "    if(len(image_matrix.shape) < 3):\n",
    "        cMap = cm.Greys_r\n",
    "    image.imsave(image_file, image_matrix, cmap=cMap)\n",
    "    \n",
    "def flatten_image_matrix(image_matrix):\n",
    "    \"\"\"\n",
    "    Flatten image matrix from \n",
    "    Height by Width by Depth\n",
    "    to (Height*Width) by Depth\n",
    "    matrix.\n",
    "    \n",
    "    params:\n",
    "    image_matrix = (color) numpy.ndarray[numpy.ndarray[numpy.ndarray[float]]] or (grayscale) numpy.ndarray[numpy.ndarray[float]]\n",
    "    \n",
    "    returns:\n",
    "    flattened_values = (color) numpy.ndarray[numpy.ndarray[float]] or (grayscale) numpy.ndarray[float]    \n",
    "    \"\"\"\n",
    "    if(len(image_matrix.shape) == 3):\n",
    "        height, width, depth = image_matrix.shape\n",
    "    else:\n",
    "        height, width = image_matrix.shape\n",
    "        depth = 1\n",
    "    flattened_values = np.zeros([height*width,depth])\n",
    "    for i, r in enumerate(image_matrix):\n",
    "        for j, c in enumerate(r):\n",
    "            flattened_values[i*width+j,:] = c\n",
    "    return flattened_values\n",
    "\n",
    "def unflatten_image_matrix(image_matrix, width):\n",
    "    \"\"\"\n",
    "    Unflatten image matrix from\n",
    "    (Height*Width) by Depth to\n",
    "    Height by Width by Depth matrix.\n",
    "    \n",
    "    params:\n",
    "    image_matrix = (color) numpy.ndarray[numpy.ndarray[float]] or (grayscale) numpy.ndarray[float]\n",
    "    width = int\n",
    "    \n",
    "    returns:\n",
    "    unflattened_values = (color) numpy.ndarray[numpy.ndarray[numpy.ndarray[float]]] or (grayscale) numpy.ndarray[numpy.ndarray[float]]\n",
    "    \"\"\"\n",
    "    heightWidth = image_matrix.shape[0]\n",
    "    height = int(heightWidth / width)\n",
    "    if(len(image_matrix.shape) > 1):\n",
    "        depth = image_matrix.shape[-1]\n",
    "        unflattened_values = np.zeros([height, width, depth])\n",
    "        for i in range(height):\n",
    "            for j in range(width):\n",
    "                unflattened_values[i,j,:] = image_matrix[i*width+j,:]\n",
    "    else:\n",
    "        depth = 1\n",
    "        unflattened_values = np.zeros([height, width])\n",
    "        for i in range(height):\n",
    "            for j in range(width):\n",
    "                unflattened_values[i,j] = image_matrix[i*width+j]\n",
    "    return unflattened_values\n",
    "\n",
    "def image_difference(image_values_1, image_values_2):\n",
    "    \"\"\"\n",
    "    Calculate the total difference \n",
    "    in values between two images.\n",
    "    Assumes that both images have same\n",
    "    shape.\n",
    "    \n",
    "    params:\n",
    "    image_values_1 = (color) numpy.ndarray[numpy.ndarray[numpy.ndarray[float]]] or (grayscale) numpy.ndarray[numpy.ndarray[float]]\n",
    "    image_values_2 = (color) numpy.ndarray[numpy.ndarray[numpy.ndarray[float]]] or (grayscale) numpy.ndarray[numpy.ndarray[float]]\n",
    "    \n",
    "    returns:\n",
    "    dist = int\n",
    "    \"\"\"\n",
    "    flat_vals_1 = flatten_image_matrix(image_values_1)\n",
    "    flat_vals_2 = flatten_image_matrix(image_values_2)\n",
    "    N, depth = flat_vals_1.shape\n",
    "    dist = 0.\n",
    "    point_thresh = 0.005\n",
    "    for i in range(N):\n",
    "        if(depth > 1):\n",
    "            new_dist = sum(abs(flat_vals_1[i] - flat_vals_2[i]))\n",
    "            if(new_dist > depth * point_thresh):\n",
    "                dist += new_dist\n",
    "        else:\n",
    "            new_dist = abs(flat_vals_1[i] - flat_vals_2[i])\n",
    "            if(new_dist > point_thresh):\n",
    "                dist += new_dist\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# image_dir = 'images/'\n",
    "# image_file = 'doge_color.png'\n",
    "# values = image_to_matrix(image_dir + image_file)\n",
    "# value_vector = flatten_image_matrix(values)\n",
    "# i=0\n",
    "# for p in value_vector:\n",
    "#     print p\n",
    "#     i+=1\n",
    "#     if i==10:\n",
    "#         break\n",
    "# matrix_to_image(values,'testfile.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1: K-means clustering\n",
    "--\n",
    "20 pts\n",
    "\n",
    "One easy method for image segmentation is to simply cluster all similar data points and then replace their values with the mean value. Thus, we'll warm up by using k-means clustering to provide a baseline to compare with your segmentation (clustering will come in handy later). Fill out `k_means_cluster()` to convert the original image values matrix to its clustered counterpart. Your convergence test should be whether the assigned clusters stop changing. Note that this convergence test is rather slow. When no initial cluster means are provided, `k_means_cluster()` should choose $k$ random points from the data (without replacement) to use as cluster means.\n",
    "\n",
    "For this part of the assignment, since clustering is best used on multidimensional data we will be using the color image `doge_color.png`.\n",
    "\n",
    "You can test your implementation of k-means using our reference images in `k_means_test()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from functools import reduce\n",
    "from numpy import random\n",
    "def k_means_cluster(image_values, k=3, initial_means=None):\n",
    "    \"\"\"\n",
    "    Separate the provided RGB values into\n",
    "    k separate clusters using the k-means algorithm,\n",
    "    then return an updated version of the image\n",
    "    with the original values replaced with\n",
    "    the corresponding cluster values.\n",
    "    \n",
    "    params:\n",
    "    image_values = numpy.ndarray[numpy.ndarray[numpy.ndarray[float]]]\n",
    "    k = int\n",
    "    initial_means = numpy.ndarray[numpy.ndarray[float]] or None\n",
    "    \n",
    "    returns:\n",
    "    updated_image_values = numpy.ndarray[numpy.ndarray[numpy.ndarray[float]]]\n",
    "    \"\"\"\n",
    "    #get flattened image_vector\n",
    "    image_vector = flatten_image_matrix(image_values)\n",
    "    num_points = len(image_vector)\n",
    "    \n",
    "    #get random initial_means\n",
    "    if initial_means == None:\n",
    "        initial_means_idx = np.random.choice(num_points,k,replace=False)\n",
    "        initial_means = np.array([image_vector[i] for i in initial_means_idx])\n",
    "        \n",
    "    #setup variables\n",
    "    current_point_index = 0\n",
    "    cluster_label = [-1 for i in range(num_points)]\n",
    "    threshold = 1e-05\n",
    "    avg_movement = float(\"inf\")\n",
    "    \n",
    "    #repeat following until centroids of clusters stop moving too much\n",
    "    while(avg_movement>threshold):\n",
    "        \n",
    "        #for each point, find it's closest mean and assign that index to it\n",
    "        for current_point in image_vector:\n",
    "            #find distance to each cluster center, assign index of whichever is closest\n",
    "            min_dist = float(\"inf\")\n",
    "            for i in range(k):\n",
    "                current_center = initial_means[i]\n",
    "                distance = np.linalg.norm(current_center-current_point)\n",
    "                if distance < min_dist:\n",
    "                    min_dist = distance\n",
    "                    cluster_label[current_point_index] = i\n",
    "                    \n",
    "        current_point_index += 1\n",
    "        \n",
    "        #update the centroids for each cluster\n",
    "        avg_movement = 0\n",
    "        for centroid_index in range(k):\n",
    "            idx = [i for i,x in enumerate(cluster_label) if x==centroid_index]\n",
    "            new_centroid = np.mean([image_vector[i] for i in idx])\n",
    "            avg_movement += np.linalg.norm(new_centroid-initial_means[centroid_index]) \n",
    "            initial_means[centroid_index] = new_centroid\n",
    "        avg_movement /= k\n",
    "    \n",
    "    #once convergence has been achieved, replace all points belonging to cluster #i with centroid[i]\n",
    "    for centroid_index in range(k):\n",
    "        points_in_cluster_idx = [i for i,x in enumerate(cluster_label) if x==centroid_index]\n",
    "        for idx in points_in_cluster_idx:\n",
    "            image_vector[idx] = initial_means[centroid_index]\n",
    "            \n",
    "    updated_image_values = unflatten_image_matrix(image_vector, len(image_values[0]))\n",
    "            \n",
    "    return updated_image_values, initial_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def k_means_test():\n",
    "    \"\"\"\n",
    "    Testing your implementation\n",
    "    of k-means on the segmented\n",
    "    doge reference images.\n",
    "    \"\"\"\n",
    "    k_min = 2\n",
    "    k_max = 6\n",
    "    image_dir = 'images/'\n",
    "    image_name = 'doge_color.png'\n",
    "    image_values = image_to_matrix(image_dir + image_name)\n",
    "    # initial mean for each k value\n",
    "    initial_means = [\n",
    "        np.array([[0.90980393,0.8392157,0.65098041],[0.83137256,0.80784315,0.69411767]]),\n",
    "        np.array([[0.90980393,0.8392157,0.65098041],[0.83137256,0.80784315,0.69411767],[0.67450982,0.52941179,0.25490198]]),\n",
    "        np.array([[0.90980393,0.8392157,0.65098041],[0.83137256,0.80784315,0.69411767],[0.67450982,0.52941179,0.25490198],[0.86666667,0.8392157,0.70588237]]),\n",
    "        np.array([[0.90980393,0.8392157,0.65098041],[0.83137256,0.80784315,0.69411767],[0.67450982,0.52941179,0.25490198],[0.86666667,0.8392157,0.70588237],[0,0,0]]),\n",
    "        np.array([[0.90980393,0.8392157,0.65098041],[0.83137256,0.80784315,0.69411767],[0.67450982,0.52941179,0.25490198],[0.86666667,0.8392157,0.70588237],[0,0,0],[0.8392157,0.80392158,0.63921571]]),\n",
    "    ]\n",
    "    # test different k values to find best\n",
    "    for k in range(k_min, k_max+1):\n",
    "        updated_values, centroids = k_means_cluster(image_values, k, initial_means[k-k_min])\n",
    "        ref_image = image_dir + 'k%d_%s'%(k, image_name)\n",
    "        matrix_to_image(updated_values, ref_image)\n",
    "        ref_values = image_to_matrix(ref_image)\n",
    "        dist = image_difference(updated_values, ref_values)\n",
    "        print('Image distance = %.2f'%(dist))\n",
    "        if(int(dist) == 0):\n",
    "            print('Clustering for %d clusters produced a realistic image segmentation.'%(k))\n",
    "        else:\n",
    "            print('Clustering for %d clusters didn\\'t produce a realistic image segmentation.'%(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image distance = 0.00\n",
      "Clustering for 2 clusters produced a realistic image segmentation.\n",
      "Image distance = 0.00\n",
      "Clustering for 3 clusters produced a realistic image segmentation.\n",
      "Image distance = 0.00\n",
      "Clustering for 4 clusters produced a realistic image segmentation.\n",
      "Image distance = 0.00\n",
      "Clustering for 5 clusters produced a realistic image segmentation.\n",
      "Image distance = 0.00\n",
      "Clustering for 6 clusters produced a realistic image segmentation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Priyam\\Anaconda3\\envs\\SonGoku\\lib\\site-packages\\numpy\\core\\_methods.py:59: RuntimeWarning: Mean of empty slice.\n",
      "  warnings.warn(\"Mean of empty slice.\", RuntimeWarning)\n",
      "C:\\Users\\Priyam\\Anaconda3\\envs\\SonGoku\\lib\\site-packages\\numpy\\core\\_methods.py:71: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "k_means_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2: Implementing a Gaussian mixture model\n",
    "--\n",
    "40 pts\n",
    "\n",
    "Next, we will step beyond clustering and implement a complete Gaussian mixture model. \n",
    "\n",
    "Complete the below implementation of `GaussianMixtureModel` so that it can perform the following:\n",
    "\n",
    "1. Calculate the joint log probability of a given greyscale value. (5 points)\n",
    "2. Use expectation-maximization (EM) to train the model to represent the image as a mixture of Gaussians. (20 points) To initialize EM, set each component's mean to the grayscale value of randomly chosen pixel and variance to 1, and the mixing coefficients to a uniform distribution. Note: there are packages that can run EM automagically, but please implement your own version of EM without using these extra packages. We've set the convergence condition for you in `GaussianMixtureModel.default_convergence()`: if the new likelihood is within 10% of the previous likelihood for 10 consecutive iterations, the model has converged.\n",
    "3. Calculate the log likelihood of the trained model. (5 points)\n",
    "4. Segment the image according to the trained model. (5 points)\n",
    "5. Determine the best segmentation by iterating over model training and scoring, since EM isn't guaranteed to converge to the global maximum. (5 points)\n",
    "\n",
    "We have provided a subset of the necessary tests for this part and will provide the rest soon. \n",
    "\n",
    "Note that training the model and calculating the log likelihood of the model are both time-intensive operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint 1: for the entire assignment you should log probabilities to avoid underflow (when multiplying lots of probabilities in sequence you can end up at 0). The log form of the Gaussian probability of scalar value $x$ is:\n",
    "\n",
    "$$ln (N(x | \\mu, \\sigma)) = -0.5ln(2\\pi \\sigma^2) - \\frac{(x-\\mu)^{2}}{2\\sigma^2} $$\n",
    "\n",
    "where $\\mu$ is the mean and $\\sigma$ is standard deviation.\n",
    "\n",
    "You can calculate the sum of log probabilities by using `scipy.misc.logsumexp()`. For example, `logsumexp([-2,-3])` will return the same result as `numpy.log(numpy.exp(-2)+numpy.exp(-3))`.\n",
    "\n",
    "Hint 2: Rather than using lists of lists, you will find it much easier to store your data in `numpy.array` arrays, which you can instantiate using the command\n",
    "\n",
    "    matrix = numpy.zeros([rows, columns])\n",
    "\n",
    "where `rows` is the number of rows and `columns` the number of columns in your matrix. `numpy.zeros()` generates a matrix of the specified size containing 0s at each row/column cell. You can access cells with the syntax `matrix[2,3]` which will return the value in row 2 and column 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def default_convergence(prev_likelihood, new_likelihood, conv_ctr, conv_ctr_cap=10):\n",
    "    \"\"\"\n",
    "    Default condition for increasing\n",
    "    convergence counter: \n",
    "    new likelihood deviates less than 10%\n",
    "    from previous likelihood.\n",
    "\n",
    "    params:\n",
    "    prev_likelihood = float\n",
    "    new_likelihood = float\n",
    "    conv_ctr = int\n",
    "    conv_ctr_cap = int\n",
    "\n",
    "    returns:\n",
    "    conv_ctr = int\n",
    "    converged = boolean\n",
    "    \"\"\"\n",
    "    \n",
    "    increase_convergence_ctr = (abs(prev_likelihood) * 0.9 < \n",
    "                                abs(new_likelihood) < \n",
    "                                abs(prev_likelihood) * 1.1)\n",
    "    \n",
    "    if increase_convergence_ctr:\n",
    "        conv_ctr+=1\n",
    "    else:\n",
    "        conv_ctr =0\n",
    "        \n",
    "    return conv_ctr, conv_ctr > conv_ctr_cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from random import randint, sample\n",
    "import math\n",
    "from scipy.misc import logsumexp\n",
    "\n",
    "class GaussianMixtureModel:\n",
    "    \"\"\"\n",
    "    A Gaussian mixture model\n",
    "    to represent a provided \n",
    "    grayscale image.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, image_matrix, num_components, means=None):\n",
    "        \"\"\"\n",
    "        Initialize a Gaussian mixture model.\n",
    "        \n",
    "        params:\n",
    "        image_matrix = (grayscale) numpy.nparray[numpy.nparray[float]]\n",
    "        num_components = int\n",
    "        \"\"\"\n",
    "        self.image_matrix = image_matrix\n",
    "        self.image_vector = flatten_image_matrix(image_matrix)\n",
    "        self.num_components = num_components\n",
    "        if(means is None):\n",
    "            self.means = np.array([0]*num_components)\n",
    "        else:\n",
    "            self.means = np.array(means)\n",
    "        self.variances = np.array([0]*num_components)\n",
    "        self.mixing_coefficients = np.array([0]*num_components)\n",
    "    \n",
    "    def joint_prob(self, val):\n",
    "        \"\"\"Calculate the joint \n",
    "        log probability of a greyscale\n",
    "        value within the image.\n",
    "        \n",
    "        params:\n",
    "        val = float\n",
    "        \n",
    "        returns:\n",
    "        joint_prob = float\n",
    "        \"\"\"\n",
    "        \n",
    "        joint_prob = 0.0\n",
    "        value = np.array(val)\n",
    "        exp_value = np.zeros(self.num_components)\n",
    "        exp_coeff = np.zeros(self.num_components)\n",
    "        \n",
    "        exp_coeff = self.mixing_coefficients/np.sqrt(2*np.pi*self.variances)\n",
    "        dist = (self.means-value)**2\n",
    "        exp_value = -1.0*(dist)/(2*self.variances)\n",
    "        \n",
    "        joint_prob = logsumexp(exp_value, b=exp_coeff)\n",
    "        \n",
    "        return joint_prob\n",
    "    \n",
    "    def initialize_training(self):\n",
    "        \"\"\"\n",
    "        Initialize the training\n",
    "        process by setting each\n",
    "        component mean to a random\n",
    "        pixel's value (without replacement),\n",
    "        each component variance to 1, and\n",
    "        each component mixing coefficient\n",
    "        to a uniform value \n",
    "        (e.g. 4 components -> [0.25,0.25,0.25,0.25]).\n",
    "        \n",
    "        NOTE: this should be called before \n",
    "        train_model() in order for tests\n",
    "        to execute correctly.\n",
    "        \"\"\"\n",
    "        #initialize means to random k points from image_vector\n",
    "        num_pixels = len(self.image_vector)\n",
    "        random_idx = sample(range(num_pixels), self.num_components)\n",
    "        self.means = np.array([self.image_vector[i] for i in random_idx])\n",
    "        self.means = np.ravel(self.means)\n",
    "        \n",
    "        #initialize variance to 1\n",
    "        self.variances = np.array([1]*self.num_components)\n",
    "        \n",
    "        #initialize mixing coeff (weights)\n",
    "        self.mixing_coefficients = np.array([float(1.0/self.num_components)]*self.num_components)\n",
    "        \n",
    "    \n",
    "    def train_model(self, convergence_function=default_convergence):\n",
    "        \"\"\"\n",
    "        Train the mixture model \n",
    "        using the expectation-maximization\n",
    "        algorithm. Since each Gaussian is\n",
    "        a combination of mean and variance,\n",
    "        this will fill self.means and \n",
    "        self.variances, plus \n",
    "        self.mixing_coefficients, with\n",
    "        the values that maximize\n",
    "        the overall model likelihood.\n",
    "        \n",
    "        params:\n",
    "        convergence_function = function that returns True if convergence is reached\n",
    "        \"\"\"\n",
    "        old_likelihood = float(\"inf\")\n",
    "        conv_status = False\n",
    "        conv_ctr = 0\n",
    "        \n",
    "        while(1):\n",
    "            z_n_k = (self.mixing_coefficients*np.exp(-1.0*((self.means-self.image_vector)**2)/(2*self.variances)))/np.sqrt(2*np.pi*self.variances)\n",
    "            z_n_k = z_n_k/np.array([np.sum(z_n_k,1)]).T\n",
    "            \n",
    "            n_k = np.sum(z_n_k,0)\n",
    "            self.mixing_coefficients = n_k/len(self.image_vector)\n",
    "            self.means = np.sum(z_n_k*self.image_vector,0)/n_k\n",
    "            self.variances = np.sum(z_n_k*((self.means-self.image_vector)**2),0)/n_k\n",
    "            \n",
    "            new_likelihood = self.likelihood()\n",
    "            conv_ctr, conv_status = convergence_function(old_likelihood, new_likelihood, conv_ctr)\n",
    "#             print conv_ctr\n",
    "            if(conv_status):\n",
    "                break\n",
    "            old_likelihood = new_likelihood\n",
    "            \n",
    "        return\n",
    "            \n",
    "    \n",
    "    def segment(self):\n",
    "        \"\"\"\n",
    "        Using the trained model, \n",
    "        segment the image matrix into\n",
    "        the pre-specified number of \n",
    "        components. Returns the original \n",
    "        image matrix with the each \n",
    "        pixel's intensity replaced \n",
    "        with its max-likelihood \n",
    "        component mean.\n",
    "        \n",
    "        returns:\n",
    "        segment = numpy.ndarray[numpy.ndarray[float]]\n",
    "        \"\"\"\n",
    "        \n",
    "        z_n_k = (self.mixing_coefficients*np.exp(-1.0*((self.means-self.image_vector)**2)/(2*self.variances)))/np.sqrt(2*np.pi*self.variances)\n",
    "        z_n_k = z_n_k/np.array([np.sum(z_n_k,1)]).T \n",
    "        \n",
    "        z_n_k = z_n_k.argmax(axis=1)\n",
    "        segment = np.zeros([len(self.image_vector),1])\n",
    "        for centroid_index in range(self.num_components):\n",
    "            points_in_cluster_idx = [i for i,x in enumerate(z_n_k) if x==centroid_index]\n",
    "            for idx in points_in_cluster_idx:\n",
    "                segment[idx] = self.means[centroid_index]\n",
    "        segment = unflatten_image_matrix(np.ravel(segment), len(self.image_matrix[0]))\n",
    "                                         \n",
    "        return segment\n",
    "    \n",
    "    def likelihood(self):\n",
    "        \"\"\"Assign a log \n",
    "        likelihood to the trained\n",
    "        model based on the following \n",
    "        formula for posterior probability:\n",
    "        ln(Pr(X | mixing, mean, stdev)) = sum((n=1 to N),ln(sum((k=1 to K), mixing_k * N(x_n | mean_k, stdev_k) )))\n",
    "        \n",
    "        returns:\n",
    "        log_likelihood = float [0,1]\n",
    "        \"\"\"\n",
    "        \n",
    "        dist = (self.means-self.image_vector)**2\n",
    "        exp_value = -1.0*dist/(2*self.variances)\n",
    "        exp_coeff = self.mixing_coefficients/np.sqrt(2*np.pi*self.variances)\n",
    "        exp_coeff = np.tile(exp_coeff,(len(self.image_vector),1))\n",
    "        log_likelihood = np.sum(logsumexp(exp_value, b=exp_coeff, axis=1))\n",
    "        \n",
    "        return log_likelihood\n",
    "        \n",
    "        \n",
    "    def best_segment(self, iters):\n",
    "        \"\"\"Determine the best segmentation\n",
    "        of the image by repeatedly \n",
    "        training the model and \n",
    "        calculating its likelihood. \n",
    "        Return the segment with the\n",
    "        highest likelihood.\n",
    "        \n",
    "        params:\n",
    "        iters = int\n",
    "        \n",
    "        returns:\n",
    "        segment = numpy.ndarray[numpy.ndarray[float]]\n",
    "        \"\"\"\n",
    "        best_seg = []\n",
    "        best_likelihood = float(\"-inf\")\n",
    "        for i in range(iters):\n",
    "            print i\n",
    "            self.initialize_training()\n",
    "            self.train_model()\n",
    "            current_likelihood = self.likelihood()\n",
    "            if current_likelihood > best_likelihood:\n",
    "                best_likelihood = current_likelihood\n",
    "                best_seg = self.segment()\n",
    "        segment = best_seg\n",
    "        return segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gmm_likelihood_test():\n",
    "    \"\"\"Testing the GMM method\n",
    "    for calculating the overall\n",
    "    model probability.\n",
    "    Should return -364370.\n",
    "    \n",
    "    returns:\n",
    "    likelihood = float\n",
    "    \"\"\"\n",
    "    image_file = 'images/party_spock.png'\n",
    "    image_matrix = image_to_matrix(image_file)\n",
    "    num_components = 5\n",
    "    gmm = GaussianMixtureModel(image_matrix, num_components)\n",
    "    gmm.initialize_training()\n",
    "    gmm.means = np.array([0.4627451, 0.10196079, 0.027450981, 0.011764706, 0.1254902])\n",
    "    likelihood = gmm.likelihood()\n",
    "    return likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-364370.25319318147"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmm_likelihood_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gmm_joint_prob_test():\n",
    "    \"\"\"Testing the GMM method\n",
    "    for calculating the joint \n",
    "    log probability of a given point.\n",
    "    Should return -0.98196.\n",
    "    \n",
    "    returns:\n",
    "    joint_prob = float\n",
    "    \"\"\"\n",
    "    image_file = 'images/party_spock.png'\n",
    "    image_matrix = image_to_matrix(image_file)\n",
    "    num_components = 5\n",
    "    gmm = GaussianMixtureModel(image_matrix, num_components)\n",
    "    gmm.initialize_training()\n",
    "    gmm.means = np.array([0.4627451, 0.10196079, 0.027450981, 0.011764706, 0.1254902])\n",
    "    test_val = 0.4627451\n",
    "    joint_prob = gmm.joint_prob(0.4627451)\n",
    "    return joint_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.98195853390140697"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmm_joint_prob_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_test_mixture(data_size, means, variances, mixing_coefficients):\n",
    "    \"\"\"\n",
    "    Generate synthetic test\n",
    "    data for a GMM based on\n",
    "    fixed means, variances and\n",
    "    mixing coefficients.\n",
    "    \n",
    "    params:\n",
    "    data_size = (int)\n",
    "    means = [float]\n",
    "    variances = [float]\n",
    "    mixing_coefficients = [float]\n",
    "    \n",
    "    returns:\n",
    "    data = np.array[float]\n",
    "    \"\"\"\n",
    "\n",
    "    data = np.zeros(data_size).flatten()\n",
    "\n",
    "    indices = np.random.choice( len(means), len(data), p=mixing_coefficients)\n",
    "\n",
    "    for i in range(len(indices)):\n",
    "        data[i] = np.random.normal(means[indices[i]], variances[indices[i]])\n",
    "\n",
    "    return np.array([data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gmm_train_test():\n",
    "    \"\"\"Test the training \n",
    "    procedure for GMM using\n",
    "    synthetic data.\n",
    "    \n",
    "    returns:\n",
    "    gmm = GaussianMixtureModel\n",
    "    \"\"\"\n",
    "\n",
    "    print( 'Synthetic example with 2 means')\n",
    "\n",
    "    num_components = 2\n",
    "    data_range = (1,1000)\n",
    "    actual_means = [2, 4]\n",
    "    actual_variances = [1]*num_components\n",
    "    actual_mixing = [.5]*num_components\n",
    "    dataset_1 = generate_test_mixture(data_range, actual_means, actual_variances, actual_mixing)\n",
    "    gmm = GaussianMixtureModel(dataset_1, num_components)\n",
    "    gmm.initialize_training()\n",
    "    # start off with faulty means\n",
    "    gmm.means = np.array([1,3])\n",
    "    initial_likelihood = gmm.likelihood()\n",
    "\n",
    "    gmm.train_model()\n",
    "    final_likelihood = gmm.likelihood()\n",
    "    likelihood_difference = final_likelihood - initial_likelihood\n",
    "    likelihood_thresh = 250\n",
    "    print likelihood_difference\n",
    "    if(likelihood_difference >= likelihood_thresh):\n",
    "        print('Congrats! Your model\\'s log likelihood improved by at least %d.'%(likelihood_thresh))\n",
    "\n",
    "    print( 'Synthetic example with 4 means:')\n",
    "\n",
    "    num_components = 4\n",
    "    actual_means = [2,4,6,8]\n",
    "    actual_variances = [1]*num_components\n",
    "    actual_mixing = [.25]*num_components\n",
    "    dataset_1 = generate_test_mixture(data_range, \n",
    "                actual_means, actual_variances, actual_mixing)\n",
    "    gmm = GaussianMixtureModel(dataset_1, num_components)\n",
    "    gmm.initialize_training()\n",
    "    # start off with faulty means\n",
    "    gmm.means = np.array([1,3,5,9])\n",
    "    initial_likelihood = gmm.likelihood()\n",
    "    gmm.train_model()\n",
    "    final_likelihood = gmm.likelihood()\n",
    "    \n",
    "    # compare likelihoods\n",
    "    likelihood_difference = final_likelihood - initial_likelihood\n",
    "    likelihood_thresh = 200\n",
    "    print likelihood_difference\n",
    "    if(likelihood_difference >= likelihood_thresh):\n",
    "        print('Congrats! Your model\\'s log likelihood improved by at least %d.'%(likelihood_thresh))\n",
    "    return gmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic example with 2 means\n",
      "294.454166124\n",
      "Congrats! Your model's log likelihood improved by at least 250.\n",
      "Synthetic example with 4 means:\n",
      "252.70740628\n",
      "Congrats! Your model's log likelihood improved by at least 200.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.GaussianMixtureModel instance at 0x0000000013E9A7C8>"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmm_train_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gmm_segment_test():\n",
    "    \"\"\"\n",
    "    Apply the trained GMM \n",
    "    to unsegmented image and\n",
    "    generate a segmented image.\n",
    "    \n",
    "    returns:\n",
    "    segmented_matrix = numpy.ndarray[numpy.ndarray[float]]\n",
    "    \"\"\"\n",
    "    image_file = 'images/party_spock.png'\n",
    "    image_matrix = image_to_matrix(image_file)\n",
    "    num_components = 3\n",
    "    gmm = GaussianMixtureModel(image_matrix, num_components)\n",
    "    gmm.initialize_training()\n",
    "    gmm.train_model()\n",
    "    segment = gmm.segment()\n",
    "    segment_num_components = len(np.unique(segment))\n",
    "    if(segment_num_components == num_components):\n",
    "        print('Congrats! Your segmentation produced an image '+\n",
    "              'with the correct number of components.')\n",
    "    return segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congrats! Your segmentation produced an image with the correct number of components.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.1450919,  0.1450919,  0.1450919, ...,  0.1450919,  0.1450919,\n",
       "         0.1450919],\n",
       "       [ 0.1450919,  0.1450919,  0.1450919, ...,  0.1450919,  0.1450919,\n",
       "         0.1450919],\n",
       "       [ 0.1450919,  0.1450919,  0.1450919, ...,  0.1450919,  0.1450919,\n",
       "         0.1450919],\n",
       "       ..., \n",
       "       [ 0.5989131,  0.5989131,  0.5989131, ...,  0.5989131,  0.5989131,\n",
       "         0.5989131],\n",
       "       [ 0.5989131,  0.5989131,  0.5989131, ...,  0.5989131,  0.5989131,\n",
       "         0.5989131],\n",
       "       [ 0.5989131,  0.5989131,  0.5989131, ...,  0.5989131,  0.5989131,\n",
       "         0.5989131]])"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmm_segment_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gmm_best_segment_test():\n",
    "    \"\"\"\n",
    "    Calculate the best segment\n",
    "    generated by the GMM and\n",
    "    compare the subsequent likelihood\n",
    "    of a reference segmentation. \n",
    "    Note: this test will take a while \n",
    "    to run.\n",
    "    \n",
    "    returns:\n",
    "    best_seg = np.ndarray[np.ndarray[float]]\n",
    "    \"\"\"\n",
    "    image_file = 'images/party_spock.png'\n",
    "    image_matrix = image_to_matrix(image_file)\n",
    "    image_matrix_flat = flatten_image_matrix(image_matrix)\n",
    "    num_components = 3\n",
    "    gmm = GaussianMixtureModel(image_matrix, num_components)\n",
    "    gmm.initialize_training()\n",
    "    iters = 10\n",
    "    # generate best segment from 10 iterations\n",
    "    # and extract its likelihood\n",
    "    best_seg = gmm.best_segment(iters)\n",
    "    matrix_to_image(best_seg, 'images/best_segment_spock.png')\n",
    "    best_likelihood = gmm.likelihood()\n",
    "    \n",
    "    # extract likelihood from reference image\n",
    "    ref_image_file = 'images/party_spock%d_baseline.png'%(num_components)\n",
    "    ref_image = image_to_matrix(ref_image_file, grays=True)\n",
    "    gmm_ref = GaussianMixtureModel(image_matrix, num_components)\n",
    "    ref_vals = ref_image.flatten()\n",
    "    ref_means = list(set(ref_vals))\n",
    "    ref_variances = [0]*num_components\n",
    "    ref_mixing = [0]*num_components\n",
    "    for i in range(num_components):\n",
    "        relevant_vals = ref_vals[ref_vals==ref_means[i]]\n",
    "        ref_mixing[i] = len(relevant_vals) / len(ref_vals)\n",
    "        ref_variances[i] = np.mean((image_matrix_flat[ref_vals==ref_means[i]] - ref_means[i])**2)\n",
    "    gmm_ref.means = np.array(ref_means)\n",
    "    gmm_ref.variances = np.array(ref_variances)\n",
    "    gmm_ref.mixing_coefficients = np.array(ref_mixing)\n",
    "    ref_likelihood = gmm_ref.likelihood()\n",
    "    \n",
    "    # compare best likelihood and reference likelihood\n",
    "    likelihood_diff = best_likelihood - ref_likelihood\n",
    "    likelihood_thresh = 1e4\n",
    "    if(likelihood_diff >= likelihood_thresh):\n",
    "        print('Congrats! Your image segmentation is an improvement over ' +\n",
    "              'the baseline by at least %.2f.'%(likelihood_thresh))\n",
    "    return best_seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "Congrats! Your image segmentation is an improvement over the baseline by at least 10000.00.\n"
     ]
    }
   ],
   "source": [
    "best_segment = gmm_best_segment_test()\n",
    "matrix_to_image(best_segment, 'best_segment.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3: Model experimentation\n",
    "--\n",
    "20 points\n",
    "\n",
    "We'll now experiment with a few methods for improving GMM performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3a: Improved initialization\n",
    "--\n",
    "12.5 points\n",
    "\n",
    "To run EM in our baseline Gaussian mixture model, we use random initialization to determine the initial values for our component means. We can do better than this!\n",
    "\n",
    "Fill in the below `GaussianMixtureModelImproved.initialize_training()` with an improvement in component initialization. Please don't use any external packages for anything other than basic calculations (e.g. `scipy.misc.logsumexp`). Note that your improvement might significantly slow down runtime, although we don't expect you to spend more than 10 minutes on initialization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: you'll probably want an unsupervised learning method to initialize your component means. Clustering is one useful example of unsupervised learning, and you may want to look at 1-dimensional methods such as [Jenks natural breaks optimization](https://en.wikipedia.org/wiki/Jenks_natural_breaks_optimization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class GaussianMixtureModelImproved(GaussianMixtureModel):\n",
    "    \"\"\"A Gaussian mixture model\n",
    "    for a provided grayscale image, \n",
    "    with improved training \n",
    "    performance.\"\"\"\n",
    "    \n",
    "    def initialize_training(self):\n",
    "        \"\"\"\n",
    "        Initialize the training\n",
    "        process by setting each\n",
    "        component mean to a random\n",
    "        pixel's value (without replacement),\n",
    "        each component variance to 1, and\n",
    "        each component mixing coefficient\n",
    "        to a uniform value \n",
    "        (e.g. 4 components -> [0.25,0.25,0.25,0.25]).\n",
    "        \"\"\"\n",
    "        self.variances = np.array([1]*self.num_components)\n",
    "        self.mixing_coefficients = np.zeros(self.num_components)\n",
    "        \n",
    "        num_pixels = len(self.image_vector)\n",
    "        k = self.num_components\n",
    "        \n",
    "        #implement k-means for this greyscale image\n",
    "        initial_means_idx = np.random.choice(num_pixels,k,replace=False)\n",
    "        initial_means = np.array([self.image_vector[i] for i in initial_means_idx])\n",
    "#         print initial_means\n",
    "        \n",
    "        #setup variables\n",
    "        current_point_index = 0\n",
    "        cluster_label = [-1 for i in range(num_pixels)]\n",
    "        threshold = 100\n",
    "        avg_movement = float(\"inf\")\n",
    "\n",
    "        #repeat following until centroids of clusters stop moving too much\n",
    "        while(avg_movement>threshold):\n",
    "\n",
    "            #for each point, find it's closest mean and assign that index to it\n",
    "            for current_point in self.image_vector:\n",
    "                #find distance to each cluster center, assign index of whichever is closest\n",
    "                min_dist = float(\"inf\")\n",
    "                for i in range(k):\n",
    "                    current_center = initial_means[i]\n",
    "                    distance = (current_center-current_point)**2\n",
    "#                     print [i,distance]\n",
    "                    if distance < min_dist:\n",
    "                        min_dist = distance\n",
    "                        cluster_label[current_point_index] = i\n",
    "\n",
    "                current_point_index += 1\n",
    "\n",
    "            #update the centroids for each cluster\n",
    "            avg_movement = 0\n",
    "            for centroid_index in range(k):\n",
    "                idx = [i for i,x in enumerate(cluster_label) if x==centroid_index]\n",
    "#                 print len(idx)\n",
    "                new_centroid = np.mean([self.image_vector[i] for i in idx])\n",
    "                avg_movement += (new_centroid - initial_means[centroid_index])**2\n",
    "                initial_means[centroid_index] = new_centroid\n",
    "            \n",
    "            avg_movement /= k\n",
    "\n",
    "        #once convergence has been achieved, replace all points belonging to cluster #i with centroid[i]\n",
    "        for centroid_index in range(k):\n",
    "            points_in_cluster_idx = [i for i,x in enumerate(cluster_label) if x==centroid_index]\n",
    "            self.mixing_coefficients[centroid_index] = len(points_in_cluster_idx)/num_pixels\n",
    "        \n",
    "        self.means = np.ravel(initial_means)\n",
    "        \n",
    "#         print self.means\n",
    "#         print self.variances\n",
    "#         print self.mixing_coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gmm_improvement_test():\n",
    "    \"\"\"\n",
    "    Tests whether the new mixture \n",
    "    model is actually an improvement\n",
    "    over the previous one: if the\n",
    "    new model has a higher likelihood\n",
    "    than the previous model for the\n",
    "    provided initial means.\n",
    "    \n",
    "    returns:\n",
    "    original_segment = numpy.ndarray[numpy.ndarray[float]]\n",
    "    improved_segment = numpy.ndarray[numpy.ndarray[float]]\n",
    "    \"\"\"\n",
    "    image_file = 'images/party_spock.png'\n",
    "    image_matrix = image_to_matrix(image_file)\n",
    "    num_components = 3\n",
    "    initial_means = [0.4627451, 0.20392157, 0.36078432]\n",
    "    # first train original model with fixed means\n",
    "    gmm = GaussianMixtureModel(image_matrix, num_components)\n",
    "    gmm.initialize_training()\n",
    "    gmm.means = np.array(initial_means)\n",
    "    gmm.train_model()\n",
    "    original_segment = gmm.segment()\n",
    "    original_likelihood = gmm.likelihood()\n",
    "    # then train improved model\n",
    "    gmm_improved = GaussianMixtureModelImproved(image_matrix, num_components)\n",
    "    gmm_improved.initialize_training()\n",
    "    gmm_improved.train_model()\n",
    "    improved_segment = gmm_improved.segment()\n",
    "    improved_likelihood = gmm_improved.likelihood()\n",
    "    # then calculate likelihood difference\n",
    "    diff_thresh = 1e3\n",
    "    likelihood_diff = improved_likelihood - original_likelihood\n",
    "    if(likelihood_diff >= diff_thresh):\n",
    "        print('Congrats! Improved model scores a likelihood that was at ' +\n",
    "              'least %d higher than the original model.'%(diff_thresh))\n",
    "    return original_segment, improved_segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congrats! Improved model scores a likelihood that was at least 1000 higher than the original model.\n"
     ]
    }
   ],
   "source": [
    "best_segment, best_segment_improved = gmm_improvement_test()\n",
    "matrix_to_image(best_segment, 'best_segment_original.png')\n",
    "matrix_to_image(best_segment_improved, 'best_segment_improved.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3b: Convergence condition\n",
    "--\n",
    "7.5 points\n",
    "\n",
    "You might be skeptical of the convergence criterion we've provided in `default_convergence()`. To test out another convergence condition, implement `new_convergence_condition()` to return true if all the new model parameters (means, variances, and mixing coefficients) are within 10% of the previous variables for 10 consecutive iterations. This will mean re-implementing `train_model()`, which you will also do below in `GaussianMixtureModelConvergence`. \n",
    "\n",
    "You can compare the two convergence functions in `convergence_condition_test()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def new_convergence_function(previous_variables, new_variables, conv_ctr, conv_ctr_cap=10):\n",
    "    \"\"\"\n",
    "    Convergence function\n",
    "    based on parameters:\n",
    "    when all variables vary by\n",
    "    less than 10% from the previous\n",
    "    iteration's variables, increase\n",
    "    the convergence counter.\n",
    "    \n",
    "    params:\n",
    "    \n",
    "    previous_variables = [numpy.ndarray[float]] containing [means, variances, mixing_coefficients]\n",
    "    new_variables = [numpy.ndarray[float]] containing [means, variances, mixing_coefficients]\n",
    "    conv_ctr = int\n",
    "    conv_ctr_cap = int\n",
    "    \n",
    "    return:\n",
    "    conv_ctr = int\n",
    "    converged = boolean\n",
    "    \"\"\"\n",
    "    \n",
    "    converged = True\n",
    "    for i in range(3):\n",
    "        converged &= np.mean(previous_variables[i])*0.9 < np.mean(new_variables[i]) < np.mean(previous_variables[i])*1.1\n",
    "    \n",
    "    if converged:\n",
    "        conv_ctr += 1\n",
    "    else:\n",
    "        conv_ctr = 0\n",
    "        \n",
    "    converged &= conv_ctr > conv_ctr_cap\n",
    "    \n",
    "    return conv_ctr, converged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GaussianMixtureModelConvergence(GaussianMixtureModel):\n",
    "    \"\"\"\n",
    "    Class to test the \n",
    "    new convergence function\n",
    "    in the same GMM model as\n",
    "    before.\n",
    "    \"\"\"\n",
    "\n",
    "    def train_model(self, convergence_function=new_convergence_function):\n",
    "        old_var = [self.means, self.variances, self.mixing_coefficients]\n",
    "        conv_status = False\n",
    "        conv_ctr = 0\n",
    "        \n",
    "        while(1):\n",
    "            z_n_k = (self.mixing_coefficients*np.exp(-1.0*((self.means-self.image_vector)**2)/(2*self.variances)))/np.sqrt(2*np.pi*self.variances)\n",
    "            z_n_k = z_n_k/np.array([np.sum(z_n_k,1)]).T\n",
    "            \n",
    "            n_k = np.sum(z_n_k,0)\n",
    "            self.mixing_coefficients = n_k/len(self.image_vector)\n",
    "            self.means = np.sum(z_n_k*self.image_vector,0)/n_k\n",
    "            self.variances = np.sum(z_n_k*((self.means-self.image_vector)**2),0)/n_k\n",
    "            \n",
    "            new_var = [self.means, self.variances, self.mixing_coefficients]\n",
    "            conv_ctr, conv_status = convergence_function(old_var, new_var, conv_ctr)\n",
    "            if(conv_status):\n",
    "                break\n",
    "            old_var = new_var\n",
    "            \n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convergence_condition_test():\n",
    "    \"\"\"\n",
    "    Compare the performance of \n",
    "    the default convergence function\n",
    "    with the new convergence function.\n",
    "    \n",
    "    return:\n",
    "    default_convergence_likelihood = float\n",
    "    new_convergence_likelihood = float\n",
    "    \"\"\"\n",
    "\n",
    "    image_file = 'images/party_spock.png'\n",
    "#     print image_file\n",
    "    image_matrix = image_to_matrix(image_file)\n",
    "    num_components = 3\n",
    "    initial_means = [0.4627451, 0.10196079, 0.027450981]\n",
    "    \n",
    "    # first test original model\n",
    "    gmm = GaussianMixtureModel(image_matrix, num_components)\n",
    "    gmm.initialize_training()\n",
    "    gmm.means = np.array(initial_means)\n",
    "    gmm.train_model()\n",
    "    default_convergence_likelihood = gmm.likelihood()\n",
    "    \n",
    "    # now test new convergence model\n",
    "    gmm_new = GaussianMixtureModelConvergence(image_matrix, num_components)\n",
    "    gmm_new.initialize_training()\n",
    "    gmm_new.means = np.array(initial_means)\n",
    "    gmm_new.train_model()\n",
    "    new_convergence_likelihood = gmm_new.likelihood()\n",
    "    \n",
    "    # test convergence difference\n",
    "    convergence_diff = abs(default_convergence_likelihood - new_convergence_likelihood)\n",
    "    convergence_thresh = 200\n",
    "    print convergence_diff\n",
    "    if(convergence_diff >= convergence_thresh):\n",
    "        print('Congrats! The likelihood difference between the original '\n",
    "              + 'and the new convergence models should be at least %.2f'%(convergence_thresh))\n",
    "    return default_convergence_likelihood, new_convergence_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3584.43060592\n",
      "Congrats! The likelihood difference between the original and the new convergence models should be at least 200.00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(92659.526094801258, 89075.09548887884)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convergence_condition_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 4: Bayesian information criterion\n",
    "-- \n",
    "20 points\n",
    "\n",
    "In our previous solutions, our only criterion for choosing a model was whether it maximizes the posterior likelihood regardless of how many parameters this requires. As a result, the \"best\" model may simply be the model with the most parameters, which would be overfit to the training data.\n",
    "\n",
    "To avoid overfitting, we can use the [Bayesian information criterion](https://en.wikipedia.org/wiki/Bayesian_information_criterion) (a.k.a. BIC) which penalizes models based on the number of parameters they use. In the case of the Gaussian mixture model, this is equal to the number of components times the number of variables per component (mean, variance and mixing coefficient) = 3\\*components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4a: Implement BIC\n",
    "--\n",
    "5 points\n",
    "\n",
    "Implement `bayes_info_criterion()` to calculate the BIC of a trained `GaussianMixtureModel`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bayes_info_criterion(gmm):\n",
    "    \n",
    "    BIC = -2.0*gmm.likelihood() + np.log(len(gmm.image_vector))*(3*gmm.num_components)\n",
    "    \n",
    "    return BIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bayes_info_test():\n",
    "    \"\"\"\n",
    "    Test for your\n",
    "    implementation of\n",
    "    BIC on fixed GMM values.\n",
    "    Should be about 727045.\n",
    "    \n",
    "    returns:\n",
    "    BIC = float\n",
    "    \"\"\"\n",
    "    \n",
    "    image_file = 'images/party_spock.png'\n",
    "    image_matrix = image_to_matrix(image_file)\n",
    "    num_components = 3\n",
    "    initial_means = [0.4627451, 0.10196079, 0.027450981]\n",
    "    gmm = GaussianMixtureModel(image_matrix, num_components)\n",
    "    gmm.initialize_training()\n",
    "    gmm.means = np.copy(initial_means)\n",
    "    BIC = bayes_info_criterion(gmm)\n",
    "    return BIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "727045.37420266762"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bayes_info_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4b: Test BIC\n",
    "-- \n",
    "15 points\n",
    "\n",
    "Now implement `BIC_model_test()`, in which you will use the BIC and likelihood to determine the optimal number of components in the Party Spock image. Use the original `GaussianMixtureModel` for your models. Iterate from k=2 to k=7 and use the provided means to train a model that minimizes its BIC and a model that maximizes its likelihood.\n",
    "\n",
    "Then, fill out `BIC_likelihood_question()` to return the number of components in both the min-BIC and the max-likelihood model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def BIC_likelihood_model_test():\n",
    "    \"\"\"Test to compare the \n",
    "    models with the lowest BIC\n",
    "    and the highest likelihood.\n",
    "    \n",
    "    returns:\n",
    "    min_BIC_model = GaussianMixtureModel\n",
    "    max_likelihood_model = GaussianMixtureModel\n",
    "    \"\"\"\n",
    "    comp_means = [\n",
    "        [0.023529412, 0.1254902],\n",
    "        [0.023529412, 0.1254902, 0.20392157],\n",
    "        [0.023529412, 0.1254902, 0.20392157, 0.36078432],\n",
    "        [0.023529412, 0.1254902, 0.20392157, 0.36078432, 0.59215689],\n",
    "        [0.023529412, 0.1254902, 0.20392157, 0.36078432, 0.59215689, 0.71372563],\n",
    "        [0.023529412, 0.1254902, 0.20392157, 0.36078432, 0.59215689, 0.71372563, 0.964706]\n",
    "    ]\n",
    "    \n",
    "    image_file = 'images/party_spock.png'\n",
    "    image_matrix = image_to_matrix(image_file)\n",
    "    min_BIC = float(\"inf\")\n",
    "    min_BIC_model = []\n",
    "    max_likelihood = float(\"-inf\")\n",
    "    max_likelihood_model = []\n",
    "\n",
    "    for i in range(2,8):\n",
    "        num_components = i\n",
    "        gmm = GaussianMixtureModel(image_matrix, num_components)\n",
    "        gmm.initialize_training()\n",
    "        gmm.means = np.array(comp_means[i-2])\n",
    "        gmm.train_model()\n",
    "        curr_BIC = bayes_info_criterion(gmm)\n",
    "        curr_likelihood = gmm.likelihood()\n",
    "        if curr_BIC < min_BIC:\n",
    "            min_BIC = curr_BIC\n",
    "            min_BIC_model = gmm\n",
    "        if curr_likelihood > max_likelihood:\n",
    "            max_likelihood = curr_likelihood\n",
    "            max_likelihood_model = gmm\n",
    "        \n",
    "    return min_BIC_model, max_likelihood_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def BIC_likelihood_question():\n",
    "    \"\"\"\n",
    "    Choose the best number of\n",
    "    components for each metric\n",
    "    (min BIC and maximum likelihood).\n",
    "    \n",
    "    returns:\n",
    "    pairs = dict\n",
    "    \"\"\"\n",
    "    BIC_model, likelihood_model = BIC_likelihood_model_test()\n",
    "    bic = BIC_model.num_components\n",
    "    likelihood = likelihood_model.num_components\n",
    "    pairs = {\n",
    "        'BIC' : bic,\n",
    "        'likelihood' : likelihood \n",
    "    }\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BIC': 7, 'likelihood': 7}"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BIC_likelihood_question()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You're done! Submit this iPython notebook on T-Square by November 20th at 9:35 AM."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
